{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Pytorch Segmentation + CAM\n",
    "For this assignment, in the first part, we're going to use Deep Learning for a new task: semantic segmentation. In the second part, you will interpret networks with the class activation map (CAM) as discussed in classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short recap of  semantic segmentation\n",
    "The goal of semantic segmentation is to classify each pixel of the image to a corresponding class of what the pixel represent. One major diference between semantic segmentation and classification is that for semantic segmentation, model output a label for each pixel instead of a single label for the whole image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMP Facade Database and Visualize Samples\n",
    "In this assignment, we use a new dataset named: CMP Facade Database for semantic segmentation. This dataset is made up with 606 rectified images of the facade of various buildings. The facades are from different cities arount the world with different architectural styles.\n",
    "\n",
    "CMP Facade DB include 12 semantic classes:\n",
    "\n",
    "* facade \n",
    "* molding\n",
    "* cornice\n",
    "* pillar\n",
    "* window\n",
    "* door\n",
    "* sill\n",
    "* blind\n",
    "* balcony\n",
    "* shop\n",
    "* deco\n",
    "* background\n",
    "\n",
    "In this assignment, we should use a model to classify each pixel in images to one of these 12 classes.\n",
    "\n",
    "For more detail about CMP Facade Dataset, if you are intereseted, please check: https://cmp.felk.cvut.cz/~tylecr1/facade/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "idxs = [1, 2, 5, 6, 7, 8]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(12, 8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    pic = plt.imread(\"dataset/base/cmp_b000{}.jpg\".format(idx))\n",
    "    label = plt.imread(\"dataset/base/cmp_b000{}.png\".format(idx), format=\"PNG\")\n",
    "\n",
    "    axes[0][i].axis('off')\n",
    "    axes[0][i].imshow(pic)\n",
    "    axes[0][i].set_title(\"Raw Image\")\n",
    "\n",
    "    axes[1][i].imshow(label)\n",
    "    axes[1][i].axis('off')\n",
    "    axes[1][i].set_title(\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataloader and Set Up Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "# import os.path as osp\n",
    "\n",
    "from FCN.dataset import CMP_Facade_DB\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "def get_full_list(\n",
    "    root_dir,\n",
    "    base_dir=\"base\",\n",
    "    extended_dir=\"extended\",\n",
    "):\n",
    "    data_list = []\n",
    "    for name in [base_dir, extended_dir]:\n",
    "        data_dir = os.path.join(\n",
    "            root_dir, name\n",
    "        )\n",
    "        data_list += sorted(\n",
    "            os.path.join(data_dir, img_name) for img_name in\n",
    "            filter(\n",
    "                lambda x: x[-4:] == '.jpg',\n",
    "                os.listdir(data_dir)\n",
    "            )\n",
    "        )\n",
    "    return data_list\n",
    "\n",
    "TRAIN_SIZE = 500\n",
    "VAL_SIZE = 30\n",
    "TEST_SIZE = 70\n",
    "full_data_list = get_full_list(\"dataset\")\n",
    "\n",
    "train_data_set = CMP_Facade_DB(full_data_list[: TRAIN_SIZE])\n",
    "val_data_set = CMP_Facade_DB(full_data_list[TRAIN_SIZE: TRAIN_SIZE + VAL_SIZE])\n",
    "test_data_set = CMP_Facade_DB(full_data_list[TRAIN_SIZE + VAL_SIZE:])\n",
    "\n",
    "print(\"Training Set Size:\", len(train_data_set))\n",
    "print(\"Validation Set Size:\", len(val_data_set))\n",
    "print(\"Test Set Size:\", len(test_data_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data_set, batch_size=1, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data_set, batch_size=1, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data_set, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Networks for Semantic Segmentation\n",
    "\n",
    "Here we are going to explore the classical work: \"Fully Convolutional Networks for Semantic Segmentation\"(FCN).\n",
    "\n",
    "In FCN, the model uses the Transpose Convolution layers, which we've already learned during the lecture, to recover high resolution feature maps. For the overall introduction of Transpose Convolution and Fully Convolutional Networks, please review the lecture recording and lecture slides on Canvas(Lecture 10).\n",
    "\n",
    "Here we do not cover all the details in FCN. Please check the original paper: https://arxiv.org/pdf/1411.4038.pdf for more details.\n",
    "\n",
    "Besides of transpose Convolution, there are also some differences compared with the models we've been working on:\n",
    "\n",
    "* Use 1x1 Convolution to replace fully connected layers to output score for each class.\n",
    "* Use skip connection to combine high-level feature and local feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: FCN-32s (20%)\n",
    "\n",
    "In this section, we first try to implement simple version of FCN without skip connection (i.e., FCN-32s) with VGG-16 as the backbone. \n",
    "\n",
    "Compared with VGG-16, FCN-32s \n",
    "* replaces the fully connecteed layers with 1x1 convolution \n",
    "* adds a Transpose Convolution at the end to output dense prediction.\n",
    "\n",
    "Task:\n",
    "1. Complete FCN-32s in the notebook as instructed.\n",
    "2. Train FCN-32s for 10 epochs and record the best model. Visualize the prediction results and report the test accuracy.\n",
    "3. Train FCN-32s for 20 epochs with pretrained VGG-16 weights and record the best model. Visualize the prediction results and report the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Complete the FC-32s architecture:\n",
    "\n",
    "The following Conv use kernel size = 3, padding = 1, stride =1 (except for conv1_1 where conv1_1 should use padding = 100)\n",
    "\n",
    "* [conv1_1(3,64)-relu] -> [conv1_2(64,64)-relu] -> [maxpool1(2,2)] \n",
    "* [conv2_1(64,128)-relu] -> [conv2_2(128,128)-relu] -> [maxpool2(2,2)]\n",
    "* [conv3_1(128,256)-relu] -> [conv3_2(256,256)-relu] ->[conv3_3(256,256)-relu] ->  [maxpool3(2,2)]\n",
    "* [conv4_1(256,512)-relu] -> [conv4_2(512,512)-relu] ->[conv4_3(512,512)-relu] ->  [maxpool4(2,2)]\n",
    "* [conv5_1(512,512)-relu] -> [conv5_2(512,512)-relu] ->[conv5_3(512,512)-relu] ->  [maxpool5(2,2)]\n",
    "\n",
    "The following Conv use stride = 1, padding = 0 (KxK denotes kernel size, dropout probability=0.5)\n",
    "* [fc6=conv7x7(512, 4096)-relu-dropout2d]\n",
    "* [fc7=conv1x1(4096, 4096)-relu-dropout2d]\n",
    "* [score=conv1x1(4096, num_classes)]\n",
    "\n",
    "The transpose convolution: kernal size = 64, stride = 32, bias = False\n",
    "* [transpose_conv(n_class, n_class)]\n",
    "\n",
    "**Hint: The output of the transpose convolution might not have the same shape as the input, \n",
    "    take [19: 19 + input_image_width], [19: 19 + input_image_height] for width and height dimension \n",
    "    of the output to get the output with the same shape as the input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class FCN32s(nn.Module):\n",
    "    def __init__(self, n_class=21):\n",
    "        super(FCN32s, self).__init__()\n",
    "        ################################################################################\n",
    "        # TODO: Implement the layers for FCN32s.                                       #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def get_upsampling_weight(self, in_channels, out_channels, kernel_size):\n",
    "        \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "        factor = (kernel_size + 1) // 2\n",
    "        if kernel_size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:kernel_size, :kernel_size]\n",
    "        filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "               (1 - abs(og[1] - center) / factor)\n",
    "        weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                          dtype=np.float64)\n",
    "        weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "        return torch.from_numpy(weight).float()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.zero_()\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = self.get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ################################################################################\n",
    "        # TODO: Implement the forward pass for FCN32s.                                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        return h\n",
    "\n",
    "    \n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data = l1.weight.data.view(l2.weight.size())\n",
    "            l2.bias.data = l1.bias.data.view(l2.bias.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Train FCN-32s from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCN.trainer import Trainer\n",
    "\n",
    "model32 = FCN32s(n_class=12)\n",
    "model32.to(device)\n",
    "\n",
    "best_model = Trainer(\n",
    "    model32,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCN.trainer import visualize\n",
    "visualize(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train FCN-32s with the pretrained VGG16 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from FCN.trainer import Trainer\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "model32_pretrain = FCN32s(n_class=12)\n",
    "model32_pretrain.copy_params_from_vgg16(vgg16)\n",
    "model32_pretrain.to(device)\n",
    "\n",
    "best_model_pretrain = Trainer(\n",
    "    model32_pretrain,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCN.trainer import visualize\n",
    "visualize(best_model_pretrain, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: FCN-8s(30%)\n",
    "\n",
    "In this section, we explore with another technique introduced in FCN paper: Skip Connection.\n",
    "\n",
    "Task: Read the paper and understand the skip connection, then \n",
    "1. Complete FCN-8s in the notebook as instructed.\n",
    "2. Train the network for 20 epochs with pretrained VGG-16 weights and record the best model. Visualize the prediction results and report the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we provide the structure of FCN-8s, the variant of FCN with skip connections.\n",
    "\n",
    "FCN-8s architecture:\n",
    "\n",
    "The following Conv use kernel size = 3, padding = 1, stride =1 (except for conv1_1 where conv1_1 should use padding = 100)\n",
    "\n",
    "**As you can see, the structure of this part is the same as FCN-32s**\n",
    "\n",
    "* [conv1_1(3,64)-relu] -> [conv1_2(64,64)-relu] -> [maxpool1(2,2)] \n",
    "* [conv2_1(64,128)-relu] -> [conv2_2(128,128)-relu] -> [maxpool2(2,2)]\n",
    "* [conv3_1(128,256)-relu] -> [conv3_2(256,256)-relu] ->[conv3_3(256,256)-relu] ->  [maxpool3(2,2)]\n",
    "* [conv4_1(256,512)-relu] -> [conv4_2(512,512)-relu] ->[conv4_3(512,512)-relu] ->  [maxpool4(2,2)]\n",
    "* [conv5_1(512,512)-relu] -> [conv5_2(512,512)-relu] ->[conv5_3(512,512)-relu] ->  [maxpool5(2,2)]\n",
    "\n",
    "The following Conv use stride = 1, padding = 0 (KxK denotes kernel size, dropout probability=0.5)\n",
    "* [fc6=conv7x7(512, 4096)-relu-dropout2d]\n",
    "* [fc7=conv1x1(4096, 4096)-relu-dropout2d]\n",
    "* [score=conv1x1(4096, num_classes)]\n",
    "\n",
    "The Additional Score Pool use kernel size = 1, stride = 1, padding = 0\n",
    "* [score_pool_3 =conv1x1(256, num_classes)]\n",
    "* [score_pool_4 =conv1x1(512, num_classes)]\n",
    "\n",
    "The transpose convolution: kernal size = 4, stride = 2, bias = False\n",
    "* [upscore1 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "The transpose convolution: kernal size = 4, stride = 2, bias = False\n",
    "* [upscore2 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "The transpose convolution: kernal size = 16, stride = 8, bias = False\n",
    "* [upscore3 = transpose_conv(n_class, n_class)]\n",
    "\n",
    "\n",
    "Different from FCN-32s which has only single path from input to output, there are multiple data path from input to output in FCN-8s.\n",
    "\n",
    "The following graph is from original FCN paper, you can also find the graph there.\n",
    "\n",
    "![\"Architecture Graph\"](files/arch.png)\n",
    "\"Layers are shown as grids that reveal relative spatial coarseness. Only pooling and prediction layers are shown; intermediate convolution layers (including converted fully connected layers) are omitted. \" ---- FCN\n",
    "\n",
    "Detailed path specification:\n",
    "\n",
    "* score_pool_3\n",
    "    * input: output from layer \"pool3\"\n",
    "    * take [9: 9 + upscore2_width], [9: 9 + upscore2_height]\n",
    "    \n",
    "* score_pool_4,\n",
    "    * input: output from layer \"pool4\"\n",
    "    * take [5: 5 + upscore1_width], [5: 5 + upscore1_height]\n",
    "\n",
    "\n",
    "* upscore1\n",
    "    * input: output from layer \"score\"\n",
    "\n",
    "* upscore2:\n",
    "    * input: output from layer \"score_pool_4\" + output from layer \"upscore1\"\n",
    "\n",
    "* upscore3:\n",
    "    * input: output from layer \"score_pool_3\" + output from layer \"upscore2\"\n",
    "    * take [31: 31 + input_image_width], [31: 31 + input_image_height]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCN8s(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class=12):\n",
    "        super(FCN8s, self).__init__()\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the layers for FCN8s.                                        #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def get_upsampling_weight(self, in_channels, out_channels, kernel_size):\n",
    "        \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
    "        factor = (kernel_size + 1) // 2\n",
    "        if kernel_size % 2 == 1:\n",
    "            center = factor - 1\n",
    "        else:\n",
    "            center = factor - 0.5\n",
    "        og = np.ogrid[:kernel_size, :kernel_size]\n",
    "        filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "               (1 - abs(og[1] - center) / factor)\n",
    "        weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                          dtype=np.float64)\n",
    "        weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "        return torch.from_numpy(weight).float()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.zero_()\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = self.get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        ################################################################################\n",
    "        # TODO: Implement the forward pass for FCN8s.                                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             #\n",
    "        ################################################################################\n",
    "\n",
    "        return h\n",
    "\n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features = [\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "            self.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "            self.conv3_2, self.relu3_2,\n",
    "            self.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "            self.conv4_2, self.relu4_2,\n",
    "            self.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "            self.conv5_2, self.relu5_2,\n",
    "            self.conv5_3, self.relu5_3,\n",
    "            self.pool5,\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data.copy_(l1.weight.data)\n",
    "                l2.bias.data.copy_(l1.bias.data)\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n",
    "            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCN.trainer import Trainer\n",
    "import torchvision\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "model8 = FCN8s(n_class=12)\n",
    "model8.copy_params_from_vgg16(vgg16)\n",
    "model8.to(device)\n",
    "\n",
    "best_model_fcn8s = Trainer(\n",
    "    model8,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader, \n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FCN.trainer import visualize\n",
    "visualize(best_model_fcn8s, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3:  Questions(20%):\n",
    "\n",
    "#### Question 1: Compare the FCN-32s training from scratch with the FCN-32s with pretrained weights? What do you observe? Does pretrained weights help? Why? Please be as specific as possible.\n",
    "\n",
    "### Your Answer:\n",
    "\n",
    "\n",
    "#### Question 2: Compare the performance and visualization of FCN-32s and FCN-8s (both with pretrained weights). What do you observe? Which performs better? Why? Please be as specific as possible.\n",
    "\n",
    "### Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Class Activation Maps (30%)\n",
    "\n",
    "In this section, we are going to interpret neural networks decisions with the technique class activation maps (CAM). As discussed in the class, the idea is that one can highlight the image region that is important to the prediction. \n",
    "\n",
    "The resnet-18 uses global average pooling for downsampling layer 4 features and then applies an FC layer to predict the class probabilities. We select the class with the highest probability as our best guess and we denote the corresponding FC weight as $w$. \n",
    "\n",
    "Let $f_4(x,y)$ denote the layer 4 feature at spatial location (x,y). Now we can directly apply the learned FC weight $w$ to $f_4(x,y)$ to get the network prediction for this spatial location $CAM(x,y)$. $CAM$ can be obtained by repeating this for all spatial locations.\n",
    "\n",
    "You may refer to the paper (http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf) for more details. In this part, we are going to use the pretrained resnet18 as the backbone.\n",
    "\n",
    "Task: understand the approach, then\n",
    "1. For each image, show the top-1 class prediction and probability.\n",
    "2. For each image, plot the CAM using layer4 features and fc weights (corresponding to the top-1 prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_pred(logit):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        logit: (1, 1000) # the predicted logits from resnet18\n",
    "    Output:\n",
    "        cls_idx: (1, ) # the class index with highest probability\n",
    "    \"\"\"\n",
    "\n",
    "    # load the imagenet category list\n",
    "    LABELS_file = 'files/imagenet-simple-labels.json'\n",
    "    with open(LABELS_file) as f:\n",
    "        classes = json.load(f)\n",
    "\n",
    "    ################################################################################\n",
    "    # TODO: \n",
    "    #      1. Use softmax to get the class prediction probability from logits\n",
    "    #      2. Use torch.sort to get the top-1 class prediction probability (top1_prob) \n",
    "    #         and the corresponding class index (top1_idx)\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "    \n",
    "    # output the prediction\n",
    "    print('top1 prediction: {:.3f} -> {}'.format(top1_prob, classes[top1_idx]))\n",
    "\n",
    "    return top1_idx\n",
    "\n",
    "\n",
    "def returnCAM(feature_conv, weight_fc, idx):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        feature_conv: (1, 512, 7, 7) # layer4 feature\n",
    "        weight_fc: (1000, 512) # fc weight\n",
    "        idx: (1, ) # predicted class index\n",
    "    Output:\n",
    "        output_cam: (256, 256)\n",
    "    \"\"\"\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    \n",
    "    ################################################################################\n",
    "    # TODO: Implement CAM\n",
    "    #     1. the product of the layer4 features and the fc weight corresponding to \n",
    "    #        the top-1 class prediction\n",
    "    #     2. convert to cam_img of shape (7,7) and value range [0, 255]\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "    \n",
    "    # resize cam image to (256,256)\n",
    "    output_cam = cv2.resize(cam_img, size_upsample)\n",
    "\n",
    "    return output_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from CAM.resnet import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load model\n",
    "net = resnet18(pretrained=True)\n",
    "net.eval()\n",
    "\n",
    "# image normalization\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize((224,224)),\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load the imagenet category list\n",
    "LABELS_file = 'files/imagenet-simple-labels.json'\n",
    "with open(LABELS_file) as f:\n",
    "    classes = json.load(f)\n",
    "\n",
    "# load test image files/bike.jpg, files/\n",
    "for image_file in ['files/bike.jpg', 'files/cat.jpg']:\n",
    "    img = Image.open(image_file)\n",
    "    img_tensor = preprocess(img)\n",
    "\n",
    "    # extract predicted logits and layer4 feature\n",
    "    logits, layer4_feat = net(img_tensor.unsqueeze(0))\n",
    "    layer4_feat = layer4_feat.detach().numpy()\n",
    "\n",
    "    # predicted top-1 class, needs to complete the function\n",
    "    cls_idx = get_cls_pred(logits)\n",
    "\n",
    "    ################################################################################\n",
    "    # TODO: extract the weight of fc layer and convert from torch.tensor to numpy.array                                                         #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    weight_fc =  # weight_fc is of shape (1000, 512)\n",
    "    \n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "\n",
    "\n",
    "    # generate class activation mapping for the top1 prediction\n",
    "    CAMs = returnCAM(layer4_feat, weight_fc, cls_idx)\n",
    "\n",
    "    # render the CAM and output\n",
    "    img = cv2.imread(image_file)\n",
    "    height, width, _ = img.shape\n",
    "    heatmap = cv2.applyColorMap(cv2.resize(CAMs,(width, height)), cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.3 + img * 0.5\n",
    "    plt.imshow(result[:,:,::-1]/255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
